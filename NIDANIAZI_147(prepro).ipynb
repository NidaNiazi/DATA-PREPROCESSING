{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "29541ac4",
   "metadata": {},
   "source": [
    "# Required Libraries for Data Analysis and Preprocessing\n",
    "# Pandas\n",
    "Purpose: Used for data manipulation and analysis. It provides data structures like DataFrames to handle structured data efficiently.\n",
    "# NumPy\n",
    "Purpose: Provides support for numerical operations and array handling. It is often used in conjunction with Pandas for efficient data manipulation.\n",
    "# Matplotlib\n",
    "Purpose: A plotting library for creating static, interactive, and animated visualizations in Python.\n",
    "# Seaborn\n",
    "Purpose: Built on top of Matplotlib, it provides a high-level interface for drawing attractive statistical graphics, making it easier to visualize data.\n",
    "# SciPy\n",
    "Purpose: Contains modules for optimization, integration, interpolation, eigenvalue problems, and other scientific computations. It is useful for advanced mathematical operations, such as Z-score calculations.\n",
    "# Scikit-learn\n",
    "Purpose: A powerful library for machine learning that provides simple and efficient tools for data mining and data analysis. It includes various algorithms and preprocessing utilities.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79c16259",
   "metadata": {},
   "outputs": [],
   "source": [
    "#this is the way to import your libraries if not previously installed\n",
    "pip install pandas numpy matplotlib seaborn scipy scikit-learn statsmodels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5be3403",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "\n",
    "# Step 1: Load the data\n",
    "data = pd.read_csv('data.csv')  # Replace 'data.csv' with your dataset\n",
    "\n",
    "# Step 2: Inspect the data\n",
    "print(\"Shape of the dataset:\", data.shape)\n",
    "print(\"\\nFirst five rows of the dataset:\\n\", data.head())\n",
    "print(\"\\nData types of each column:\\n\", data.info())\n",
    "print(\"\\nNumber of duplicate rows:\", data.duplicated().sum())\n",
    "\n",
    "# Step 3: Summary statistics\n",
    "print(\"\\nSummary statistics:\\n\", data.describe())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71f5079e",
   "metadata": {},
   "source": [
    "# Steps Before Preprocessing:\n",
    "Data Collection\n",
    "Gather data from different sources like CSV, Excel files, databases, APIs, or web scraping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21f0a41c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load data from a CSV file\n",
    "data = pd.read_csv('data.csv')\n",
    "\n",
    "# Load data from an Excel file\n",
    "data = pd.read_excel('data.xlsx')\n",
    "\n",
    "# Load data from an SQL database\n",
    "import sqlite3\n",
    "conn = sqlite3.connect('database.db')\n",
    "data = pd.read_sql_query(\"SELECT * FROM table_name\", conn)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16be23e0",
   "metadata": {},
   "source": [
    "# Inspect the Data\n",
    "\n",
    "Check the shape of the data to understand how many rows and columns you are dealing with.\n",
    "Preview the data to get a sense of its structure.\n",
    "Understand column names, types of features (categorical or numerical), and inspect any glaring issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf6e3b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the shape of the data (rows, columns)\n",
    "print(data.shape)\n",
    "\n",
    "# Preview the first few rows of the data\n",
    "print(data.head())\n",
    "\n",
    "# Check data types of each column\n",
    "print(data.info())\n",
    "\n",
    "# Check for duplicate rows\n",
    "print(data.duplicated().sum())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0af839db",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e84368cf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "aef10681",
   "metadata": {},
   "source": [
    "# 1. Handling Missing Values\n",
    "Imputation: Replace missing values with the mean, median, or mode, or use more complex techniques like KNN imputation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "003ccaab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "dataset['column_name'] = imputer.fit_transform(dataset[['column_name']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1694f27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping: Remove rows or columns with missing values\n",
    "dataset.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94a579cc",
   "metadata": {},
   "source": [
    "# 2. Handling Categorical Variables\n",
    "One-Hot Encoding: Convert categorical values into binary columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a322ab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.get_dummies(dataset, columns=['category_column'])\n",
    "# Label Encoding: Convert categories into numerical labels\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "dataset['category_column'] = label_encoder.fit_transform(dataset['category_column'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecad609c",
   "metadata": {},
   "source": [
    "# 3. Scaling Features\n",
    "Standardization: Rescale the data so that it has a mean of 0 and standard deviation of 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32bd1f64",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "dataset[['column1', 'column2']] = scaler.fit_transform(dataset[['column1', 'column2']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe0e5726",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Min-Max Scaling: Scale values to a range of 0 to 1\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "dataset[['column1', 'column2']] = scaler.fit_transform(dataset[['column1', 'column2']])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87542ac9",
   "metadata": {},
   "source": [
    "# 4. Handling Outliers\n",
    "Capping/Flooring: Replace extreme values with a fixed value, like 1st and 99th percentiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec7c224e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['column'] = dataset['column'].clip(lower=dataset['column'].quantile(0.01), upper=dataset['column'].quantile(0.99))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09431400",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Z-score method: Remove data points that are far from the mean.\n",
    "from scipy import stats\n",
    "dataset = dataset[(np.abs(stats.zscore(dataset['column'])) < 3)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "499333d9",
   "metadata": {},
   "source": [
    "# 5. Feature Engineering\n",
    "Feature Creation: Create new features based on existing ones.\n",
    "python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffb8a675",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['new_feature'] = dataset['column1'] / dataset['column2']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19950558",
   "metadata": {},
   "source": [
    "# 6. Dimensionality Reduction\n",
    "PCA (Principal Component Analysis): Reduce the number of features while preserving variance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c58cc1dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=2)\n",
    "reduced_data = pca.fit_transform(dataset)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51d3da12",
   "metadata": {},
   "source": [
    "# 7. Data Splitting\n",
    "Train-Test Split: Split data into training and testing sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6da20ccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e917f31c",
   "metadata": {},
   "source": [
    "# 8. Encoding Dates/Time\n",
    "Extracting Date Components: If you have a date column, extract the year, month, or day.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afcaa932",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['year'] = dataset['date_column'].dt.year\n",
    "dataset['month'] = dataset['date_column'].dt.month\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb6f8107",
   "metadata": {},
   "source": [
    "# 9. Dealing with Imbalanced Data\n",
    "Oversampling: Use techniques like SMOTE to generate more samples from the minority class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d45a6b44",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "smote = SMOTE()\n",
    "X_resampled, y_resampled = smote.fit_resample(X, y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b659c6d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Undersampling: Randomly sample from the majority class to balance the data.\n",
    "\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "under_sampler = RandomUnderSampler()\n",
    "X_resampled, y_resampled = under_sampler.fit_resample(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0f44b79",
   "metadata": {},
   "source": [
    "# FOLLOWING IS THE PREPROCESSING TECHNIQUES AS ONE BIG CODE FOR BETTER UNDERSTANDING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59cd8b13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Install Required Libraries\n",
    "# You can uncomment the following line to install the libraries if you haven't already\n",
    "# !pip install pandas numpy matplotlib seaborn scikit-learn imbalanced-learn\n",
    "\n",
    "# Step 2: Import Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# Step 3: Load the Data\n",
    "data = pd.read_csv('data.csv')  # Replace with your dataset file\n",
    "print(\"Initial data shape:\", data.shape)\n",
    "\n",
    "# Step 4: Create Dependent and Independent Variable Factors\n",
    "# Assuming 'target' is the name of your target variable\n",
    "X = data.drop('target', axis=1)  # Independent variables\n",
    "y = data['target']  # Dependent variable\n",
    "print(\"Independent variables shape:\", X.shape)\n",
    "print(\"Dependent variable shape:\", y.shape)\n",
    "\n",
    "# Step 5: Handle Missing Values\n",
    "# Using SimpleImputer to fill missing values\n",
    "imputer = SimpleImputer(strategy='mean')  # Replace with 'median' or 'most_frequent' as needed\n",
    "X_imputed = imputer.fit_transform(X)\n",
    "print(\"Missing values handled. Imputed data shape:\", X_imputed.shape)\n",
    "\n",
    "# Step 6: Data Encoding (One Hot Encoding)\n",
    "X_encoded = pd.get_dummies(X_imputed, drop_first=True)\n",
    "print(\"Data after One Hot Encoding shape:\", X_encoded.shape)\n",
    "\n",
    "# Step 7: Splitting Data into Train and Test Set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_encoded, y, test_size=0.2, random_state=42)\n",
    "print(\"Training set shape:\", X_train.shape)\n",
    "print(\"Test set shape:\", X_test.shape)\n",
    "\n",
    "# Step 8: Feature Scaling\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "print(\"Feature scaling applied.\")\n",
    "\n",
    "# Step 9: Outlier Detection (Using Z-score)\n",
    "z_scores = np.abs(stats.zscore(X_train_scaled))\n",
    "outliers = np.where(z_scores > 3)[0]  # Identify outliers\n",
    "print(f\"Indices of outliers in the training set: {outliers}\")\n",
    "\n",
    "# Optional: Remove outliers from training data\n",
    "# X_train_scaled = np.delete(X_train_scaled, outliers, axis=0)\n",
    "# y_train = np.delete(y_train.values, outliers, axis=0)\n",
    "\n",
    "# Step 10: Convert Ordinal Data into Numeric Values\n",
    "ordinal_columns = ['ordinal_column']  # Replace with actual ordinal column names\n",
    "ordinal_encoder = OrdinalEncoder()\n",
    "X_train_scaled[:, [X_train.columns.get_loc(col) for col in ordinal_columns]] = ordinal_encoder.fit_transform(\n",
    "    X_train_scaled[:, [X_train.columns.get_loc(col) for col in ordinal_columns]]\n",
    ")\n",
    "X_test_scaled[:, [X_test.columns.get_loc(col) for col in ordinal_columns]] = ordinal_encoder.transform(\n",
    "    X_test_scaled[:, [X_test.columns.get_loc(col) for col in ordinal_columns]]\n",
    ")\n",
    "\n",
    "# Step 11: Data Binning (if applicable)\n",
    "# Example: Creating bins for a continuous variable\n",
    "# X_train_scaled['binned_column'] = pd.cut(X_train_scaled['continuous_column'], bins=5, labels=False)  # Replace with your column\n",
    "# X_test_scaled['binned_column'] = pd.cut(X_test_scaled['continuous_column'], bins=5, labels=False)  # Replace with your column\n",
    "\n",
    "# Step 12: Managing Under and Over Sampling\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_resampled, y_train_resampled = smote.fit_resample(X_train_scaled, y_train)\n",
    "print(\"Resampled training set shape:\", X_train_resampled.shape)\n",
    "\n",
    "# Final Overview\n",
    "print(\"\\nData preprocessing completed successfully.\")\n",
    "print(\"Final training data shape:\", X_train_resampled.shape)\n",
    "print(\"Final test data shape:\", X_test_scaled.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c457adb4",
   "metadata": {},
   "source": [
    "# Difference between EDA and Pre-Processing :\n",
    "# EDA:\n",
    "\n",
    "Visualization: Create charts (histograms, box plots, scatter plots) to explore data distribution and relationships between features.\n",
    "Summary Statistics: Calculate mean, median, standard deviation, correlations to summarize the data.\n",
    "Outlier Detection: Detect potential anomalies in the data through visual or statistical means.\n",
    "Distribution Analysis: Study the distribution of numerical variables (e.g., normal, skewed).\n",
    "# Preprocessing:\n",
    "\n",
    "Handling Missing Data: Imputation (mean, median, mode) or removing missing entries.\n",
    "Encoding Categorical Data: One-hot encoding, label encoding to convert categorical variables into numerical ones.\n",
    "Feature Scaling: Normalization or standardization to bring all features to a common scale.\n",
    "Handling Outliers: Removing or capping extreme values.\n",
    "Data Transformation: Log transformations, binning, or polynomial features to transform data for better performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b9b2544",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
